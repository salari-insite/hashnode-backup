---
title: "The Matrix Is Everywhere: Linear Algebra Concepts Relevant to Computer Science"
slug: the-matrix-is-everywhere-linear-algebra-concepts-relevant-to-computer-science
cover: https://cdn.hashnode.com/res/hashnode/image/stock/unsplash/iar-afB0QQw/upload/a17458d20a3a815e5372c9f31a2d811e.jpeg

---

While linear algebra originated as a branch of "pure mathematics," its applications have since expanded significantly beyond the mathematical domain. In this article, I'll go in-depth on explaining the core linear algebra concepts and theories that play a significant role in computer science.

---

# Introduction

**This article is part of a series called "Log Base Two"; this series is dedicated to exploring the intricate relationship between mathematics and computer science.**

Just to give a bit of background on my experience in math, I've **excelled** in a total of 10 college courses in math:

1. Single-variable differential calculus
    
2. Single-variable integral calculus
    
3. Multi-variable differential and integral calculus
    
4. Multi-variable differential and integral calculus of vector-valued functions
    
5. Linear Algebra
    
6. Differential equations
    
7. Probability and statistics
    
8. Data analysis using R programming language
    
9. Multivariate statistics
    
10. Discrete math
    

For a large majority of those 10 classes, I ended up with A's.

Although, before college, I struggled a lot with math in the classroom, even though math had always been my favorite subject since I was in elementary school. I found it hard to truly grasp the concepts taught because my high school teachers were too focused on skill assessment. Instead of focusing on the actual concept being taught, I was too worried about how I was going to pass the test the following week.

Once I got to college, I realized that your math grade (or any grade for that matter) truly doesn't define your potential. Further, I'd argue that your success in math *education* is heavily influenced by the quality of the teacher/professor's teaching.

My mission with this series is to show other people that math isn't so scary if you break down mathematical concepts into digestible bouts of information and investigate use cases of math in your primary field of interest (in this case, computer science). Along the way, I hope to also supplement your math education with quality content. I believe that **everyone** is capable of learning and truly loving mathematics.

**New articles in this series are posted every Thursday. If you have any suggestions for a specific topic I should write about, please comment it down below!**

Stay tuned for next week's article where I explain the use of eigenvalues and eigenvectors in machine learning algorithms!

Without further ado, let's get to today's article.

---

# What Even is Linear Algebra?

Linear algebra is a subfield of mathematics that concentrates on linear systems, matrices, and vectors. Through the employment of these three fundamental elements, a multitude of advancements have been achieved in various domains, both within and beyond the sphere of mathematics.

## Why Haven't I Heard About Linear Algebra Before?

If you're still at the beginning of your computer science journey, you might've not been exposed to linear algebra quite yet.

Typically, the first opportunity to take a course in linear algebra is in your 2nd or 3rd year of college. Additionally, colleges usually require you to pass single-variable calculus before being able to enroll in linear algebra.

On top of this, most non-STEM majors in college don't require students to pass any college-level math past single-variable calculus and statistics.

The funny thing is that linear algebra isn't *harder* than calculus. In fact, A majority of my classmates in university, myself included, believe that linear algebra is a bit *easier* than calculus.

Also, linear algebra doesn't directly use any tools or concepts from calculus, so you can't even make the argument that you first need to gain prerequisite information from calculus to understand linear algebra.

So if linear algebra isn't harder than calculus, and there's no prerequisite information gained from calculus, then why does it seem so exclusive and "higher level"?

While I don't have an official answer, I have an opinion as to why this is the case.

I believe that it comes down to two main factors:

* Linear algebra's industrial uses are not as widespread as the branches of math that are taught before it
    
    * While **both** calculus and linear algebra are pretty much only used in STEM and a few other industries (finance, economics, some arts), their uses within those industries are not equal
        
* Linear algebra is a "mature" math
    
    * I know I said that linear algebra is sometimes seen as easier than calculus, but linear algebra's *concepts* are much more abstract than concepts in single-variable calculus
        

With all that being said, there's no problem with getting a head start on learning linear algebra if you're still in school right now.

And if you're going the self-taught route with computer science, then all the more reason to self-teach the math used in computer science!

---

# Key Linear Algebra Concepts Used in Computer Science

I've carefully curated a list of 9 concepts in linear algebra that are used across various branches of computer science including machine learning, computer graphics, cryptography, image processing, information theory, and much more.

For each of these concepts, I'll provide enough general information so you can understand the big picture, and I'll also mention an example of a branch of computer science that uses the concept.

Also, I ordered the concepts in chronological order to have the concepts build on top of each other and improve the flow of the article.

For reference, here are the concepts that I'll be covering:

* **Linear systems**
    
* **Matrices and Gaussian elimination**
    
* **Vectors and vector spaces**
    
* **Orthogonality**
    
* **Determinants**
    
* **Eigenvalues and eigenvectors**
    
* **Diagonalization and symmetric matrices**
    
* **Linear transformations**
    
* **Singular value decomposition**
    

## I. Linear systems

A linear system can be defined as an assembly of multiple <mark>linear equations</mark> that share a common set of variables. These equations are interconnected, and their solutions are represented by the values of the variables that <mark>simultaneously</mark> satisfy all the equations within the system.

One application of linear systems is in **numerical analysis**. Numerical analysis involves using algorithms that can be used to find approximate numerical solutions to problems where the exact numerical solution is impossible to solve or is too complicated to solve.

### Linear equations

A linear equation is an equation that consists of *n* variables each with a degree of 1.

The general form of a linear equation is:

$$a_1 x_1 + a_2 x_2 + \ldots + a_n x_n = b$$

The 3 main components of a linear equation are:

* a set of coefficients
    

$$\{a_1, a_2, \ldots, a_n\}$$

* a set of variables
    

$$\{x_1, x_2, \ldots, x_n\}$$

* a constant *b*
    

In linear systems of equations, each equation contains the same set of variables but doesn't necessarily have the same set of coefficients.

Putting together a system of *m* linear equations takes the general form:

$$a_{11} x_1 + a_{12} x_2 + \ldots + a_{1n} x_n = b_1$$

$$a_{21} x_1 + a_{22} x_2 + \ldots + a_{2n} x_n = b_2$$

$$\vdots$$

$$a_{m1} x_1 + a_{m2} x_2 + \ldots + a_{mn} x_n = b_m$$

### Simultaneous solution

The main idea behind a system of equations is that all equations within the system can be solved *simultaneously*.

By doing so, it becomes possible to find a unique solution, or a set of solutions, that satisfies all the given equations, thereby providing a comprehensive understanding of the problem at hand.

## II. Matrices and Gaussian elimination

As a way to simply store a linear system in a mathematical object that can be operated on, we use a <mark>matrix</mark>. Not only do matrices provide a way to store a linear system, but they simplify the process of being able to solve linear systems through a method called <mark>Gaussian elimination</mark>.

Matrices are found all throughout computer science. One of the most prevalent uses of matrices in computer science is computer graphics. In animation, for example, you can use a matrix to define a particular transformation for an object. These matrix transformations allow for movement, rotation, and changing the size of an object by defining a new set of coordinates for the object to be represented in the animation.

### Matrices

A matrix is simply an array of numbers arranged into *n* rows and *m* columns. Each row in a matrix represents all of the variable's coefficients for each equation, and each column represents all of the coefficients applied to a particular variable in the system.

$$$$ \begin{pmatrix} a & b & c\\ d & e & f\\ g & h & i \end{pmatrix} $$$$

When we refer to a matrix by its size, we use the general notation: *n x m.* The *n* represents the number of rows, and the *m* represents the number of columns. For the above matrix, we'd say it's a 3 x 3 matrix.

The above matrix only contains the coefficients in a linear system. If we wanted to include the linear system's constants in a matrix, the standard notation for doing so is with an augmented matrix:

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1694496706146/45bcdcd8-1913-4f46-8824-298bdff3bd75.png align="center")

In an augmented matrix, the values on the right side of the vertical bar represent the constants in the linear equations. Typically we use augmented matrices when solving linear systems so that we can arrive at a solution that involves the constants in the equations.

### Gaussian Elimination

One method of solving a linear system is by using something called Gaussian elimination on a matrix.

Gaussian elimination is essentially an algorithm where you apply row operations to a matrix to reduce the matrix to what's called *row echelon form*. A matrix is in row echelon form when it satisfies the following conditions:

* All rows filled with all zeros are at the bottom of the matrix
    
* The leading entry (leftmost non-zero entry) of every non-zero filled row is equal to 1
    
* The leading entry of every non-zero filled row is to the right of the leading entry of the row above it
    

Once a matrix is in row echelon form, you can start to find the values of all the variables using basic algebra.

I know all that seems confusing, but when you see it in practice, it might make some more sense.

Here's an example of applying Gaussian elimination on an augmented matrix to achieve row echelon form:

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1694499096373/ed40d79a-1f44-4f5e-91b1-02b56ed3b207.png align="center")

In the above example, I first chose to multiply all the values in the 1st row by half, since that would make the first entry in the 1st row equal to 1.

For the next row operation, I noticed that the 2nd row's first entry was already 0, meaning my only goal was to make that 7 in the 2nd row turn into a 1. As such, I multiplied the entire row by 1/7.

As you can see, the 3rd row needed some more work than the first two rows.

For the 3rd row, I first added the 1st row's values to it, so that I could turn the first entry into a 0.

My next task was to make the 3rd row's second entry a 0, so I made the 3rd row negative and then added the 2nd row's values multiplied by 1.5.

My last and final step was to simply make the 3rd row's third entry equal to 1, so I just had to multiply the row by -14/41.

As you can see, after applying all of those row operations on the matrix, we end up with a matrix that is in row echelon form.

The resulting matrix doesn't have any zero-filled rows, so we don't have to worry about satisfying the condition that all zero-filled rows are at the bottom of the matrix.

For the other two conditions, the leading entry in each row is equal to 1, and the leading entry in each row is to the right of the leading entry in the row above it.

Now that we have the matrix in row echelon form, we can easily calculate the values of each of the variables using basic algebra.

The 3rd row tells us that the third variable is equal to -36/41.

We can now plug the third variable's value into the 2nd row, and find that the second variable is equal to 19/41.

Finally, we can plug the second variable and third variable's values into the first row, and find that the first variable is equal to 122/41.

## III. Vectors and vector spaces

A <mark>vector</mark> is a quantity that is described by both a magnitude and direction. A <mark>vector space</mark> is a set of vectors, where each vector can be added to another vector in the vector space and the resultant vector is still in the vector space. Additionally, each vector in a vector space can be multiplied by a scalar quantity (a fancy way of saying "number") and the resultant vector would still be in the vector space.

One example of vectors and vector spaces being used in computer science is in web information retrieval systems used by search engines. Vectors are used to represent webpage documents and search queries. By using a vector space model, search engines can return relevant results to a user's search query.

### Vectors

For example, if you have a plot a point ***A*** at (0, 0) in the Cartesian coordinate system, and then plot another point ***B*** at (4, 6), the vector from ***A*** to *B* would be:

$$\vec{v} = \begin{pmatrix} 4 \\ 6 \end{pmatrix}$$

## Orthogonality

## Determinants

## Eigenvalues and eigenvectors

## Diagonalization and symmetric matrices

## Linear transformations

## Singular value decomposition

---